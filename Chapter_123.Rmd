---
title: "MBA5011 Multivariate Analysis: Model-Based Statistics"
subtitle: "Week 1 Tutorial"
output: 
  word_document:
    toc: yes
---
# Chapter 1
## Preface
![Rabbi Loew and Golem by Mikoláš Aleš, 1899](/Users/chengyahan/Desktop/Work/Sunny/Multivariate Analysis/textbook figures/1.1.png)

_Ultimately Judah was forced to destroy the golem, as its combination of extraordinary power with clumsiness eventually led to innocent deaths. Wiping away one letter from the inscription emet to spell instead met, “death,” Rabbi Judah decommissioned the robot._


Obtain Session info
```{r}
sessionInfo()
```


# Chapter 2
## Preface

McElreath described the thrust of this chapter this way:

In this chapter, you will begin to build Bayesian models. The way that Bayesian models learn from evidence is arguably optimal in the small world. When their assumptions approximate reality, they also perform well in the large world. But large world performance has to be demonstrated rather than logically deduced.


## 2.1 The garden of forking data
### 2.1.1 Counting possibilities

Throughout this project, we’ll use the tidyverse for data wrangling.

```{r setup, message=FALSE, warning=FALSE}
library(tidyverse)
```

If you are new to tidyverse-style syntax,possibly the oddest component is the pipe `(i.e., %>%)`.

the other big thing to be aware of is `tibbles`. For our purposes, think of a tibble as a data object with two dimensions defined by rows and columns. And importantly, tibbles are just special types of `data.frames`.

So, if we’re willing to code the marbles as 0 = “white” 1 = “blue”, we can arrange the possibility data in a tibble as follows.

```{r}
d <- tibble(p_1 = 0, # every different combination of marbles selected randomly for a bag
         p_2 = rep(1:0, times = c(1, 3)), # there are 4 different combination
         p_3 = rep(1:0, times = c(2, 2)), # 0 = “white” 1 = “blue”
         p_4 = rep(1:0, times = c(3, 1)), 
         p_5 = 1)

head(d) # check the format of tibble

```

and you might depict the possibility data in a plot.

```{r out.width = "2400px"}
d %>% 
  gather() %>% # turn multiple columns and collapses into key-value pairs
  mutate(x = rep(1:4, times = 5), # adds new variables and preserves existing
         possibility = rep(1:5, each = 4)) %>% 
  
  ggplot(aes(x = x, y = possibility, # specify x- and y-axis
             fill = value %>% as.character())) + # fill dots with color (value)
  geom_point(shape = 21, size = 5) + # set the shape and size of dots
  scale_fill_manual(values = c("white", "navy")) + # fill colors (0 = "white"; 1 = "navy")
  scale_x_continuous(NULL, breaks = NULL) + # delete x-axis label
  coord_cartesian(xlim = c(.75, 4.25),
                  ylim = c(.75, 5.25)) + # zooming out the plot
  theme(legend.position = "none") # delete legend
```

![Figure 2.2 The 64 possible paths generated by assuming the bag contains one blue and three white marbles. (p.22)](/Users/chengyahan/Desktop/Work/Sunny/Multivariate Analysis/textbook figures/2.2.png)


If you walk that out a little, you can structure the data required to approach Figure 2.2.

```{r}
d <-
  tibble(position = c((1:4^1) / 4^0, 
                      (1:4^2) / 4^1, 
                      (1:4^3) / 4^2),
         draw     = rep(1:3, times = c(4^1, 4^2, 4^3)),
         fill     = rep(c("b", "w"), times = c(1, 3)) %>% 
           rep(., times = c(4^0 + 4^1 + 4^2)))
```



plot "d"

```{r out.width = "2400px"}
d %>% 
  ggplot(aes(x = position, y = draw)) +
  geom_point(aes(fill = fill),
             shape = 21, size = 3) +
  scale_y_continuous(breaks = 1:3) + # we only want 3 scales
  scale_fill_manual(values  = c("navy", "white")) +
  theme(panel.grid.minor = element_blank(), # delete auxiliary line
        legend.position = "none")
```

To my mind, the easiest way to connect the dots in the appropriate way is to make two auxiliary tibbles.
```{r}
lines_1 <-
  tibble(x    = rep((1:4), each = 4), # where lines start
         xend = ((1:4^2) / 4), # where lines end 
         y    = 1, # and so on and so forth
         yend = 2)

lines_2 <-
  tibble(x    = rep(((1:4^2) / 4), each = 4),
         xend = (1:4^3)/(4^2),
         y    = 2,
         yend = 3)
```


We can use the lines_1 and lines_2 data in the plot with two geom_segment() functions.

```{r}
d %>% 
  ggplot(aes(x = position, y = draw)) +
  geom_segment(data  = lines_1,
               aes(x = x, xend = xend,
                   y = y, yend = yend),
               size  = 1/3) +
  geom_segment(data  = lines_2,
               aes(x = x, xend = xend,
                   y = y, yend = yend),
               size  = 1/3) +
  geom_point(aes(fill = fill),
             shape = 21, size = 3) +
  scale_y_continuous(breaks = 1:3) +
  scale_fill_manual(values  = c("navy", "white")) +
  theme(panel.grid.minor = element_blank(),
        legend.position = "none")
```

```{R, results = "hide", echo = FALSE}
d <-
  tibble(position = c((1:4^1) / 4^0, 
                      (1:4^2) / 4^1, 
                      (1:4^3) / 4^2),
         draw     = rep(1:3, times = c(4^1, 4^2, 4^3)))


  d <-
  d %>% 
  bind_rows(
    d, d
  ) %>% 
  # here are the fill colors
  mutate(fill = c(rep(c("w", "b"), times = c(1, 3)) %>% rep(., times = c(4^0 + 4^1 + 4^2)),
                  rep(c("w", "b"), each  = 2)       %>% rep(., times = c(4^0 + 4^1 + 4^2)),
                  rep(c("w", "b"), times = c(3, 1)) %>% rep(., times = c(4^0 + 4^1 + 4^2)))) %>% 
  # now we need to shift the positions over in accordance with draw, like before
  mutate(denominator = ifelse(draw == 1, .5,
                              ifelse(draw == 2, .5 / 4,
                                     .5 / 4^2))) %>% 
  mutate(position = position - denominator) %>% 
  # here we'll add an index for which pie wedge we're working with
  mutate(pie_index = rep(letters[1:3], each = n()/3)) %>% 
  # to get the position axis correct for pie_index == "b" or "c", we'll need to offset
  mutate(position = ifelse(pie_index == "a", position,
                           ifelse(pie_index == "b", position + 4,
                                  position + 4 * 2)))
  #define function
 move_over <- function(position, index){
  ifelse(index == "a", position,
         ifelse(index == "b", position + 4,
                position + 4 * 2)
         )
  }  
 lines_1 <-
  tibble(x    = rep((1:4), each = 4) %>% rep(., times = 3),
         xend = ((1:4^2) / 4)        %>% rep(., times = 3),
         y    = 1,
         yend = 2) %>% 
  mutate(x    = x - .5,
         xend = xend - .5 / 4^1) %>% 
  # here we'll add an index for which pie wedge we're working with
  mutate(pie_index = rep(letters[1:3], each = n()/3)) %>% 
  # to get the position axis correct for pie_index == "b" or "c", we'll need to offset
  mutate(x    = move_over(position = x,    index = pie_index),
         xend = move_over(position = xend, index = pie_index))
 lines_2 <-
  tibble(x    = rep(((1:4^2) / 4), each = 4)  %>% rep(., times = 3),
         xend = (1:4^3 / 4^2)                 %>% rep(., times = 3),
         y    = 2,
         yend = 3) %>% 
  mutate(x    = x - .5 / 4^1,
         xend = xend - .5 / 4^2) %>% 
  # here we'll add an index for which pie wedge we're working with
  mutate(pie_index = rep(letters[1:3], each = n()/3)) %>% 
  # to get the position axis correct for pie_index == "b" or "c", we'll need to offset
  mutate(x    = move_over(position = x,    index = pie_index),
         xend = move_over(position = xend, index = pie_index))
 
 d <- 
  d %>% 
  mutate(remain = c(#pie_index == "a"
                    rep(0:1, times = c(1, 3)),
                    rep(0,   times = 4),
                    rep(1:0, times = c(1, 3)) %>% 
                      rep(., times = 3),
                    rep(0,   times = 4 * 4),
                    rep(c(0, 1, 0), times = c(1, 3, 4 * 3)) %>% 
                      rep(., times = 3),
                    # pie_index == "b"
                    rep(0:1, each = 2),
                    rep(0,   times = 4 * 2),
                    rep(1:0, each = 2) %>% 
                      rep(., times = 2),
                    rep(0,   times = 4 * 4 * 2),
                    rep(c(0, 1, 0, 1, 0), times = c(2, 2, 2, 2, 8)) %>% 
                      rep(., times = 2),
                    # pie_index == "c",
                    rep(0:1, times = c(3, 1)),
                    rep(0,   times = 4 * 3),
                    rep(1:0, times = c(3, 1)), 
                    rep(0,   times = 4 * 4 * 3),
                    rep(0:1, times = c(3, 1)) %>% 
                      rep(., times = 3),
                    rep(0,   times = 4)
                    )
         )

lines_1 <-
  lines_1 %>% 
  mutate(remain = c(rep(0,   times = 4),
                    rep(1:0, times = c(1, 3)) %>% 
                      rep(., times = 3),
                    rep(0,   times = 4 * 2),
                    rep(1:0, each  = 2) %>% 
                      rep(., times = 2),
                    rep(0,   times = 4 * 3),
                    rep(1:0, times = c(3, 1))
                    )
         )

lines_2 <-
  lines_2 %>% 
  mutate(remain = c(rep(0,   times = 4 * 4),
                    rep(c(0, 1, 0), times = c(1, 3, 4 * 3)) %>% 
                      rep(., times = 3),
                    rep(0,   times = 4 * 8),
                    rep(c(0, 1, 0, 1, 0), times = c(2, 2, 2, 2, 8)) %>% 
                      rep(., times = 2),
                    rep(0,   times = 4 * 4 * 3),
                    rep(0:1, times = c(3, 1)) %>% 
                      rep(., times = 3),
                    rep(0,   times = 4)
                    )
         )
```
```{R out.width = "3600px"}
d %>% 
  ggplot(aes(x = position, y = draw)) +
  geom_vline(xintercept = c(0, 4, 8), color = "white", size = 1/2) +
  geom_segment(data  = lines_1,
               aes(x = x, xend = xend,
                   y = y, yend = yend,
                   alpha = remain %>% as.character()),
               size  = 1/3) +
  geom_segment(data  = lines_2,
               aes(x = x, xend = xend,
                   y = y, yend = yend,
                   alpha = remain %>% as.character()),
               size  = 1/3) +
  geom_point(aes(fill = fill, size = draw, alpha = remain %>% as.character()),
             shape = 21) +
  scale_size_continuous(range = c(2, 1.5)) +
  scale_alpha_manual(values = c(1/10, 1)) +
  scale_fill_manual(values  = c("navy", "white")) +
  scale_x_continuous(limits = c(0, 12),     breaks = NULL) +
  scale_y_continuous(limits = c(0.75, 3.5), breaks = NULL) +
  theme(panel.grid = element_blank(),
        legend.position = "none") +
  coord_polar() +
  labs(x = NULL, y = NULL)
 
```

### 2.1.2 Using prior information

_We may have prior information about the relative plausibility of each conjecture. This prior information could arise from knowledge of how the contents of the bag were generated. It could also arise from previous data. Or we might want to act as if we had prior information, so we can build conservatism into the analysis. Whatever the source, it would help to have a way to use prior information. Luckily there is a natural solution: Just multiply the prior count by the new count. (p. 25)_

Now suppose we draw another marble from the bag to get another observation:`blue`.First we count the numbers of ways each conjecture could produce the new observation,`blue`.Then we multiply each of these new counts by the previous numbers of ways for each conjecture. In table form:


```{r, echo=FALSE}
n_blue <- function(x){
  rowSums(x == "b")
} # self-defined function to calculate how many blue marbles

n_white <- function(x){
  rowSums(x == "w")
} # self-defined function to calculate how many white marbles

t <-
  # different combinations of 4 marbles 
  tibble(p_1 = rep(c("w", "b"), times = c(1, 4)),
         p_2 = rep(c("w", "b"), times = c(2, 3)),
         p_3 = rep(c("w", "b"), times = c(3, 2)),
         p_4 = rep(c("w", "b"), times = c(4, 1))) %>% 
  mutate(`draw 1: blue`  = n_blue(.), # ways to produce "first blue marble"
         `draw 2: white` = n_white(.), # ways to produce "second white marble"
         `draw 3: blue`  = n_blue(.)) %>%  # ways to produce "third blue marble"
  mutate(`ways to produce` = `draw 1: blue` * `draw 2: white` * `draw 3: blue`) 
         # multiply 3 ways above = ways to produce "blue, white, blue"

```

```{r}
(
  t <-
    t %>% 
    rename(`previous counts` = `ways to produce`,
           `ways to produce` = `draw 1: blue`) %>% 
    select(p_1:p_4, `ways to produce`, `previous counts`) %>% 
    mutate(`new count` = `ways to produce` * `previous counts`)
)
```


The new counts in the right-hand column above summarize all the evidence for each conjecture.


### 2.1.3 From counts to probability
Finally, we construct probabilities by standardizing the plausibility so that the sum of the plausibilities for all possible conjectures will be one.
<size=10><center>$plausibility\ of\ p\ after\ D_{new} = \frac{ways\ p\ can\ produce D_{new}\times \ prior plausubulity of p}{sum of the products}$</center>

A worked example is needed for this to really make sense. So consider again the table from before, now updated using our definitions of $p$ and $plausibility$:

```{R echo=FALSE}
t %>% 
  select(p_1:p_4) %>% 
  mutate(p                      = seq(from = 0, to = 1, by = .25), # the proportion of marbles that are blue
         `ways to produce data` = c(0, 3, 8, 9, 0)) %>% 
  mutate(plausibility           = `ways to produce data` / sum(`ways to produce data`))
```

Specifically, each piece of the calculation has a direct partner in applied probability theory. These partners have stereotyped names, so it's worth learning them, as you'll see them again and again.


- A conjectured proportion of blue marbles, `p`, is usually called a `parameter` value. It's just a way of indexing possible explanations of the data.
- The relative number of ways that a value p can produce the data is usually called a `likelihood`. It is derived by enumerating all the possible data sequences that could have happened and then eliminating those sequences inconsistent with the data.
- The prior plausibility of any specific `p` is usually called the `prior probability`.
- The new, updated plausibility of any specific p is usually called the `posterior probability`.


## 2.2 Building a model

Suppose you have a globe representing our planet, the Earth. This version of the world is small enough to hold in your hands. You are curious how much of the surface is covered in water. You adopt the following strategy: You will toss the globe up in the air. When you catch it, you will record whether or not the surface under your right index finger is water or land. Then you toss the globe up in the air again and repeat the procedure. This strategy generates a sequence of surface samples from the globe. The first nine samples might look like:`"w", "l", "w", "w", "w", "l", "w", "l", "w"`,where W indicates water and L indicates land.

To get the logic moving, we need to make assumptions, and these assumptions constitute the model. Designing a simple Bayesian model benefits from a design loop with three steps.
- (1) Data story: Motivate the model by narrating how the data might arise.
- (2) Update: Educate your model by feeding it the data.
- (3) Evaluate: All statistical models require supervision, leading possibly to model revi-
sion.

We might save our globe-tossing data in a tibble
```{R}
(d <- tibble(toss = c("w", "l", "w", "w", "w", "l", "w", "l", "w")))
```


### 2.2.1 A data story
Bayesian data analysis usually means producing a story for how the data came to be. This story may be descriptive, specifying associations that can be used to predict outcomes, given observations. Or it may be causal, a theory of how come events produce other events. Typically, any story you intend to be causal may also be descriptive. But many descriptive stories are hard to interpret causally. But all data stories are complete, in the sense that they are sufficient for specifying an algorithm for simulating new data. (p. 28)

### 2.2.2 Bayesian updating

![Figure2.5. HowaBayesianmodellearns.Each toss of the globe produces an observation of water (W) or land (L). The model’s estimate of the proportion of water on the globe is a plausibility for every possible value. The lines and curves in this figure are these collections of plausibilities. In each plot, previous plausibilities (dashed curve) are updated in light of the latest observation to produce a new set of plausibilities (solid curve).](/Users/chengyahan/Desktop/Work/Sunny/Multivariate Analysis/textbook figures/2.5.png)

A Bayesian model begins with one set of plausibilities assigned to each of these possibilities. These are the prior plausibilities. Then it updates them in light of the data, to produce the posterior plausibilities. This updating process is a kind of learning, called Bayesian updating.

Here we’ll add the cumulative number of trials, *n_trials*, and the cumulative number of successes, *n_successes* (i.e., toss == "w"), to the data.

```{R}
(
  d <-
  d %>% 
  mutate(n_trials  = 1:9,
         n_success = cumsum(toss == "w")) # cumulative sum
  )
```


We can re-correct the model with each update.

```{r}
sequence_length <- 50

d %>% 
  expand(n_trials, # expand the dataframe
         p_water = seq(from = 0, to = 1, length.out = sequence_length))   %>% 
  left_join(d, by = "n_trials") %>% # combine with tibble d
  group_by(p_water) %>% # create 50 groups
  mutate(lagged_n_success = lag(n_success, k = 1), # lag a sequence of number
         lagged_n_trials  = lag(n_trials,  k = 1)) %>% # ex: 1,2,3 -> NA, 1, 2
  ungroup() %>% 
  mutate(prior = ifelse(n_trials == 1, .5, # prior of first trial = 0.5, trials after will obey binomial dist.
                        dbinom(x    = lagged_n_success, # how many times of success before this toss
                               size = lagged_n_trials, # how many observation before this toss
                               prob = p_water)), # prob of success
         strip = str_c("n = ", n_trials), # captions of each graph
         likelihood = dbinom(x    = n_success, # likelyhood of this toss
                             size = n_trials, 
                             prob = p_water),
  ) %>% 
  # the next three lines allow us to normalize the prior and the likelihood, putting them both in a probability metric 
  group_by(n_trials) %>% 
  mutate(prior      = prior      / sum(prior), # convert counts into probabilities
         likelihood = likelihood / sum(likelihood)) %>%   
  
  ggplot(aes(x = p_water)) +
  geom_line(aes(y = prior), linetype = 2) + # add prior line (dashed)
  geom_line(aes(y = likelihood)) + # add likelihood line (solid)
  scale_x_continuous("proportion water", breaks = c(0, .5, 1)) +
  scale_y_continuous("plausibility", breaks = NULL) +
  theme(panel.grid = element_blank()) + # set grid line
  facet_wrap(~strip, scales = "free_y") # wrap 9 plots into a plot
```


The additional samples from the globe are introduced to the model, one at a time. Each dashed curve is just the solid curve from the previous plot, moving left to right and top to bottom. Every time a “W” is seen, the peak of the plausibility curve moves to the right, towards larger values of p. Every time an “L” is seen, it moves the other direction. The maximum height of the curve increases with each sample, meaning that fewer values of p amass more plausibility as the amount of evidence increases. As each new observation is added, the curve is updated consistent with all previous observations.

Notice that every updated set of plausibilities becomes the initial plausibilities for the next observation. Every conclusion is the starting point for future inference.


### 2.2.3 Evaluate
It may be that Bayesian inference is the best general purpose method of inference known. However, Bayesian inference is much less powerful than we'd like it to be. There is no approach to inference that provides universal guarantees. No branch of applied mathematics has unfettered access to reality, because math is not discovered, like the proton. Instead it is invented, like the shovel. (p. 32)


## 2.3 Components of the model

```
1. likelihood: the number of ways each conjecture could produce an observation
2. parameters: the accumulated number of ways each conjecture cold produce the entire data
3. prior     : the initial plausibility of each conjectured cause of the data
```

### 2.3.1  Likelihood
If you let the count of water be $w$ and the number of tosses be $n$, then the binomial likelihood may be expressed as:

<center>$P(w\mid n,P)=\frac{n!}{w!(n-w)!}p^w(1-p)^{n-w}$</center>

Given $w = 6$, $n = 9$ and $p = 0.5$

```{R}
dbinom(x = 6, size = 9, prob = .5)
```

McElreath suggested we change the values of prob. Let‘s do so over the parameter space.

```{R out.width = "400px"}
tibble(prob = seq(from = 0, to = 1, by = .01)) %>% 
  ggplot(aes(x = prob,
             y = dbinom(x = 6, size = 9, prob = prob))) +
  geom_line() +
  labs(x = "probability",
       y = "binomial likelihood") +
  theme(panel.grid = element_blank())
```

### 2.3.2 Parameters

It is typical to conceive of data and parameters as completely different kinds of entities. Data are measures and known; parameters are unknown and must be estimated from data. Usefully, in the Bayesian framework the distinction between a datum and a parameter is fuzzy. (p. 34)


### 2.3.3 Prior
So where do priors come from? They are engineering assumptions, chosen to help the machine learn. The flat prior in Figure 2.5 is very common, but it is hardly ever the best prior. You’ll see later in the book that priors that gently nudge the machine usually improve inference. Such priors are sometimes called regularizing or weakly informative priors. (p. 35)


### 2.3.4 Posterior
Once you have chosen a likelihood, which parameters are to be estimated, and a prior for each parameter, a Bayesian model treats the estimates as a purely logical consequence of those assumptions. For every unique combination of data, likelihood, parameters, and prior, there is a unique set of estimates.

<size=8><center>$Posterior = \frac{Likelihood \times Prior}{Average Likelihood}$</center>

The average likelihood, can be confusing. It is commonly called the `evidence` or the `probability of the data`, neither of which is a transparent name. Averaged over what? Averaged over the prior


## 2.4
Before we explore the relationship among prior, likelihood, and posterior, we need to somulate the data to plot FIGURE 2.6.

```{r}
sequence_length <- 1e3

d <-
  tibble(probability = seq(from = 0, to = 1, length.out = sequence_length)) %>% 
  expand(probability, row = c("flat", "stepped", "Laplace")) %>% 
  arrange(row, probability) %>% 
  mutate(prior = ifelse(row == "flat", 1,
                        ifelse(row == "stepped", rep(0:1, each = sequence_length/2),
                               exp(-abs(probability - .5) / .25) / ( 2 * .25))),
         likelihood = dbinom(x = 6, size = 9, prob = probability)) %>% 
  group_by(row) %>% 
  mutate(posterior = prior * likelihood / sum(prior * likelihood)) %>% 
  gather(key, value, -probability, -row) %>% 
  ungroup() %>% 
  mutate(key = factor(key, levels = c("prior", "likelihood", "posterior")),
         row = factor(row, levels = c("flat", "stepped", "Laplace"))) 
```


Recall that your Bayesian model is a machine, a figurative golem. It has built-in definitions for the likelihood, the parameters, and the prior. And then at its heart lies a motor that processes data, producing a posterior distribution,as a product of the prior distribution and likelihood.

In order to avoid unnecessary facet labels for the rows, it was easier to just make each column of the plot separately and then recombine them with *gridExtra::grid.arrange()*.

```{R echo=FALSE,message=FALSE}
p1 <-
  d %>%
  filter(key == "prior") %>% 
  ggplot(aes(x = probability, y = value)) +
  geom_line() +
  scale_x_continuous(NULL, breaks = c(0, .5, 1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "prior") +
  theme(panel.grid       = element_blank(),
        strip.background = element_blank(),
        strip.text       = element_blank()) +
  facet_wrap(~ row, scales = "free_y", ncol = 1)

p2 <-
  d %>%
  filter(key == "likelihood") %>% 
  ggplot(aes(x = probability, y = value)) +
  geom_line() +
  scale_x_continuous(NULL, breaks = c(0, .5, 1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "likelihood") +
  theme(panel.grid       = element_blank(),
        strip.background = element_blank(),
        strip.text       = element_blank()) +
  facet_wrap(~ row, scales = "free_y", ncol = 1)

p3 <-
  d %>%
  filter(key == "posterior") %>% 
  ggplot(aes(x = probability, y = value)) +
  geom_line() +
  scale_x_continuous(NULL, breaks = c(0, .5, 1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "posterior") +
  theme(panel.grid       = element_blank(),
        strip.background = element_blank(),
        strip.text       = element_blank()) +
  facet_wrap(~ row, scales = "free_y", ncol = 1)

library(gridExtra)

grid.arrange(p1, p2, p3, ncol = 3)
```



### 2.4.1 Grid approximation

One of the simplest conditioning techniques is grid approximation. While most parameters are continuous, capable of taking on an infinite number of values, it turns out that we can achieve an excellent approximation of the continuous posterior distribution by considering only a finite grid of parameter values. At any particular value of a parameter, `p′`, it’s a simple matter to compute the posterior probability: just multiply the prior probability of `p′` by the likelihood at `p′`. Repeating this procedure for each value in the grid generates an approximate picture of the exact posterior distribution. This procedure is called *grid approximation*. In this section, you’ll see how to perform a grid approximation, using simple bits of R code.

We just employed grid approximation over the last figure. In order to get nice smooth lines, we computed the posterior over 1000 evenly-spaced points on the probability space. Here we’ll prepare for Figure 2.7 with 20.

```{r}
(d <-
 tibble(p_grid            = seq(from = 0, to = 1, length.out = 20),  # define grid
        prior             = 1) %>%                                   # define prior
   mutate(likelihood      = dbinom(6, size = 9, prob = p_grid)) %>%  # compute likelihood at each value in grid
   mutate(unstd_posterior = likelihood * prior) %>%                  # compute product of likelihood and prior
   mutate(posterior       = unstd_posterior / sum(unstd_posterior))  # standardize the posterior, so it sums to 1
)
```

Here’s the right panel of Figure 2.7.

```{r}
d %>%  # plot
  ggplot(aes(x = p_grid, y = posterior)) +
  geom_point() +
  geom_line() + # add line to connect the dots
  labs(subtitle = "20 points",
       x = "probability of water",
       y = "posterior probability") +
  theme(panel.grid = element_blank())
```


Here it is with just 5 points, the left hand panel of Figure 2.7.

```{r}
tibble(p_grid            = seq(from = 0, to = 1, length.out = 5), # we only change the quantity of observations
       prior             = 1) %>%
  mutate(likelihood      = dbinom(6, size = 9, prob = p_grid)) %>%
  mutate(unstd_posterior = likelihood * prior) %>%
  mutate(posterior       = unstd_posterior / sum(unstd_posterior)) %>% 
  
  ggplot(aes(x = p_grid, y = posterior)) +
  geom_point() +
  geom_line() +
  labs(subtitle = "5 points",
       x = "probability of water",
       y = "posterior probability") +
  theme(panel.grid = element_blank())

```


### 2.4.2 Markov chain Monte Carlo

Markov chain Monte Carlo methods create samples from a possibly multi-dimensional continuous random variable, with probability density proportional to a known function. These samples can be used to evaluate an integral over that variable, as its expected value or variance.

To do Markov chain Monte Carlo (MCMC),
firstly, we need to construct our model:
<size=6><center>$w \sim bin(9\times n,p)$</center>
the prior parameters is:
<size=6><center>$p \sim unif(0,1)$</center>

In this semester, we will use the **rstan** package to help us complete the bayes model.

#### A quick-start introduction to Stan
`Stan` is a flexible modeling language that makes it straightforward to estimate a very wide range of probability models using Bayesian techniques. There are a few reasons one may want to spend the time learning Stan:

- Stan implements efficient estimation of probability models on large data sets using Hamiltonain Monte Carlo, Variational Inference and Penalised Maximum Likelihood. Stan can be called from many environments that users may use for data preparation, including R, Python, Stata, Julia and Matlab.

- Stan allows users to speed up their R code by exporting Stan functions (which are compiled C++ functions) into R. This is especially useful for users who want similar performance to Julia but are tied to R for its large library ecosystem.

- Stan is perhaps the easiest Bayesian library to learn and use, with straightforward syntax and companion libraries for easy model checking and comparison.

- Stan (alongside the workflow that is encouraged by the Stan community) forces users to think more carefully about their models than they may do otherwise. Learning Stan illuminates many aspects of statistics that you may have skipped over; it should help you think more carefully about modeling even if you do not use Stan.

- In Stan there is no penalty from using non-conjugate prior distributions, allowing far richer model specifications than under other MCMC methods.


```{R results = "hide",warning=FALSE,message=FALSE}
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
```

A `Stan` model is comprised of `code blocks`. Each block is a place for a certain task. The bold blocks below must be present in all Stan programs (even if they contain no arguments):

- `functions`, where we define functions to be used in the blocks below. This is where we will write out a random number generator that gives us draws from our assumed model.
- `data`, declares the data to be used for the model
- `transformed data`, makes transformations of the data passed in above
- `parameters`, defines the unknowns to be estimated, including any restrictions on their values.
- `transformed parameters`, often it is preferable to work with transformations of the parameters and data declared above; in this case we define them here.
- `model`, where the full probability model is defined.
- `generated quantities`, generates a range of outputs from the model (posterior predictions, forecasts, values of loss functions, etc.).

and our stan model code will be:
```{R}
stanmodel =  "
data { // declares the data to be used for the model
int n; // observations
int w; // success
}
parameters { // defines the unknowns to be estimate
real p;   
}
model {
// Define the priors
p ~ uniform(0, 1); 

// The likelihood
w ~ binomial(n, p);
}
"
```



```{R}
n9_model = list(w = 6, n = 9)  # declare data
globe_qa_9 = stan(model_code = stanmodel, # stan program
                data = n9_model, # named list of data
                cores = 1, # number of cores (could use one per chain)
                chains = 2, # number of Markov chains
                warmup = 500, # number of warmup iterations per chain
                iter = 1000) # total number of iterations per chain
```

```{R result="hide",message=FALSE,warning=FALSE}
n18_model = list(w = 6*2, n = 18)
globe_qa_18 = stan(model_code = stanmodel, data = n18_model, chains = 2, warmup = 500, iter = 1000)

n36_model = list(w = 6*4, n = 36)
globe_qa_36 = stan(model_code = stanmodel, data = n36_model, chains = 2, warmup = 500, iter = 1000)
```

Let's start by looking at the model outputs. The draws from each parameter can be neatly summarized with `print`

```{R}
print(globe_qa_9, pars = c("p"))
print(globe_qa_18, pars = c("p"))
print(globe_qa_36, pars = c("p"))
```

```{R echo=FALSE}
n_grid <- 100

tibble(p_grid                  = seq(from = 0, to = 1, length.out = n_grid) %>% rep(., times = 3),
       prior                   = 1,
       w                       = rep(c(6, 12, 24), each = n_grid),
       n                       = rep(c(9, 18, 36), each = n_grid),
       m                       = .65, # mean (stan model)
       s                       = rep(c(.13, .10, .08), each = n_grid)) %>% # sd of stan models (9, 18, 36)
  mutate(likelihood            = dbinom(w, size = n, prob = p_grid)) %>%
  mutate(unstd_grid_posterior  = likelihood * prior,
         unstd_quad_posterior  = dnorm(p_grid, m, s)) %>%
  group_by(w) %>% 
  mutate(grid_posterior        = unstd_grid_posterior / sum(unstd_grid_posterior),
         quad_posterior        = unstd_quad_posterior / sum(unstd_quad_posterior),
         n = str_c("n = ", n)) %>% 
  mutate(n = factor(n, levels = c("n = 9", "n = 18", "n = 36"))) %>% 
  
  ggplot(aes(x = p_grid)) +
  geom_line(aes(y = grid_posterior)) +
  geom_line(aes(y = quad_posterior),
            color = "grey50") +
  labs(x = "proportion water",
       y = "density") +
  theme(panel.grid = element_blank()) +
  facet_wrap(~n, scales = "free")
```


As the amount of data increases, however, the MCMC gets better. In the middle of `Figure`, the sample size is doubled to `n = 18` tosses, but with the same fraction of water, so that the mode of the posterior is in the same place. The `MCMC` looks better now, although still not great. At quadruple the data, on the right side of the figure, the two curves are nearly the same now.

# Chapter 3
## Preface
Lots of books on Bayesian statistics introduce posterior inference by using a medical testing scenario. To repeat the structure of common examples, suppose there is a blood test that correctly detects vampirism 95% of the time. This implies *Pr(positive|vampire)* = 0.95. It’s a very accurate test. It does make mistakes, though, in the form of false positives. One percent of the time, it incorrectly diagnoses normal people as vampires, implying Pr(positive|mortal) = 0.01. The final bit of information we are told is that vampires are rather rare, being only 0.1% of the population, implying Pr(vampire) = 0.001.

If you would like to know the probability someone is a vampire given they test positive to the blood-based vampire test, you compute:

<size=10><center>$Pr(vampire|positive)= \frac{Pr(positive|vampire) Pr(vampire)}{Pr(positive)}$</center>

We’ll do so within a tibble.

```{r}
library(tidyverse)

tibble(pr_positive_vampire   = .95,
       pr_positive_mortal    = .01,
       pr_vampire            = .001) %>% 
  mutate(pr_positive         = pr_positive_vampire * pr_vampire + pr_positive_mortal * (1 - pr_vampire)) %>% 
  mutate(pr_vampire_positive = pr_positive_vampire * pr_vampire / pr_positive) %>% 
  glimpse()
```


Here’s the other way of tackling the vampire problem, this time useing the frequency format.

```{r}
tibble(pr_vampire            = 100 / 100000,
       pr_positive_vampire   = 95 / 100,
       pr_positive_mortal    = 99 / 99900) %>% 
  mutate(pr_positive         = 95 + 999) %>% 
  mutate(pr_vampire_positive = pr_positive_vampire * 100 / pr_positive) %>% 
  glimpse()
```


## 3.1 Sampling from a grid-like approximate posterior

Here we use grid approximation, again, to generate samples.

```{r}
# how many grid points would you like?
n <- 1000
n_success <- 6
n_tirals  <- 9

(
  d <-
  tibble(p_grid     = seq(from = 0, to = 1, length.out = n),
       # note we're still using a flat uniform prior
         prior      = 1) %>% 
  mutate(likelihood = dbinom(n_success, size = n_tirals, prob = p_grid)) %>% 
  mutate(posterior  = likelihood * prior) %>% 
  mutate(posterior  = posterior / sum(posterior))
  )
```

Now we wish to draw 10,000 samples from this posterior.

```{r}
samples <- sample(d$p_grid, # randomly select from p_grid
                  prob = d$posterior, # probility of each p_grid
                  size = 1e4, # select 10,000 sample
                  replace = T) # sample with replacement

glimpse(samples) # get a glimpse of your data (class: vector)
```

Before plotting *samples* to make Figure 3.1, we need to convert *samples* from *vector* into *tibble data frame*.

```{r}
samples <-
  tibble(samples = sample(d$p_grid, prob = d$posterior, size = 1e4, replace = T)) %>% 
  mutate(sample_number = 1:n())

glimpse(samples)
```


```{r}

# plot to see your sample
samples %>% 
  ggplot(aes(x = sample_number, y = samples)) +
  geom_line(size = 1/10) + 
  labs(x = "sample number",
       y = "proportion of water (p)")

samples %>% 
  ggplot(aes(x = samples)) +
  geom_density(fill = "black") +
  coord_cartesian(xlim = 0:1) + # set the limit of coordinate system
  xlab("proportion of water (p)")

```


## 3.2 Sampling to summarize


Once your model produces a posterior distribution, the model’s work is done. But your work has just begun. It is necessary to summarize and interpret the posterior distribution. Exactly how it is summarized depends upon your purpose. But common questions include:

> How much posterior probability lies below some parameter value?
> How much posterior probability lies between two parameter values?
> Which parameter value marks the lower 5% of the posterior probability?
> Which range of parameter values contains 90% of the posterior probability?
> Which parameter value has highest posterior probability?


### 3.2.1 Intervals of defined boundaries

```{r, results='hide', echo=FALSE}
(d <-
 tibble(p_grid            = seq(from = 0, to = 1, length.out = 20),  # define grid
        prior             = 1) %>%                                   # define prior
   mutate(likelihood      = dbinom(6, size = 9, prob = p_grid)) %>%  # compute likelihood at each value in grid
   mutate(unstd_posterior = likelihood * prior) %>%                  # compute product of likelihood and prior
   mutate(posterior       = unstd_posterior / sum(unstd_posterior))  # standardize the posterior, so it sums to 1
)
```

Suppose I ask you for the posterior probability that the proportion of water is less than 0.5. Using the grid-approximate posterior, you can just add up all of the probabilities, where the corresponding parameter value is less than 0.5:

```{r}
d %>% 
  filter(p_grid < .5) %>% 
  summarise(sum = sum(posterior)) # sum posterior of p_grid < .5
```

You can use "&" within filter(), too.

```{r}
samples %>% 
  filter(samples > .5 & samples < .75) %>% 
  summarise(sum = n() / 1e4) # about 61% of the posterior probability lies between 0.5 and 0.75.
```


### 3.2.2 Intervals of defined mass

We’ll create the upper two panels for Figure 3.2 with geom_line(), geom_ribbon(), and a some careful filtering.

```{r}
# upper left panel
d %>% # plot posterior density and fill the interval < 0.5
  ggplot(aes(x = p_grid)) +
  geom_line(aes(y = posterior)) +
  geom_ribbon(data = d %>% filter(p_grid < .5), 
              aes(ymin = 0, ymax = posterior)) + # fill the plot between ymin and ymax
  labs(x = "proportion of water (p)",
       y = "density")


# upper right panel
d %>% # plot posterior density and fill the interval between .5 & .75
  ggplot(aes(x = p_grid)) +
  geom_line(aes(y = posterior)) +
  # note this next line is the only difference in code from the last plot
  geom_ribbon(data = d %>% filter(p_grid < .75 & p_grid > .5),
              aes(ymin = 0, ymax = posterior)) +
  labs(x = "proportion of water (p)",
       y = "density")

```


That value will come in handy for the lower left panel of Figure 3.2, so we saved it. But anyways, we could select() the samples vector, extract it from the tibble with pull(), and then pump it into quantile():

```{r}
(q_80 <- quantile(samples$samples, prob = .8))

# or
samples %>% 
  select(samples) %>% # select the column
  pull() %>% # transform the tibble into vector
  quantile(prob = .8)
```

```{r}
(q_10_and_90 <- quantile(samples$samples, prob = c(.1, .9)))
```


```{r}
# plot lower left panel
d %>% 
  ggplot(aes(x = p_grid)) +
  geom_line(aes(y = posterior)) +
  geom_ribbon(data = d %>% filter(p_grid < q_80),
              aes(ymin = 0, ymax = posterior)) +
  annotate(geom = "text",
           x = .25, y = .0025,
           label = "lower 80%") +
  labs(x = "proportion of water (p)",
       y = "density")
```

```{r}
# lower right panel
d %>% 
  ggplot(aes(x = p_grid)) +
  geom_line(aes(y = posterior)) +
  geom_ribbon(data = d %>% filter(p_grid > q_10_and_90[1] & p_grid < q_10_and_90[2]),
              aes(ymin = 0, ymax = posterior)) +
  annotate(geom = "text",
           x = .25, y = .0025,
           label = "middle 80%") +
  labs(x = "proportion of water (p)",
       y = "density")
```



Consider the posterior distribution and different intervals in Figure 3.3. This posterior is consistent with observing three waters in three tosses and a uniform (flat) prior. It is highly skewed, having its maximum value at the boundary, p = 1. You can compute it, via grid approximation, with:

```{r}
# here we update the `dbinom()` parameters
n_success <- 3
n_tirals  <- 3

# update d
d <-
  d %>% 
  mutate(likelihood = dbinom(n_success, size = n_tirals, prob = d$p_grid)) %>% 
  mutate(posterior  = likelihood * prior) %>% 
  mutate(posterior  = posterior / sum(posterior))

# here's our new samples tibble
(
  samples <- tibble(samples = sample(d$p_grid, prob = d$posterior, size = 1e4, replace = T))
)
```

check .25, .75 interval
```{r}
quantile(samples$samples, prob = c(.25, .75))
```


Now’s a good time to introduce Matthew Kay’s* tidybayes package*, which offers an array of convenience functions for Bayesian models of the type we’ll be working with in this project.

```{r}
library(tidybayes)

median_qi(samples$samples, .width = .5)
```

The tidybayes package offers a family of functions that make it easy to summarize a distribution with a measure of central tendency accompanied by intervals. With median_qi(), we asked for the median and quantile-based intervals – just like we’ve been doing with quantile().

With __.width = .5__, we indicated we wanted a quantile-based 50% interval, which was returned in the ymin and ymax columns. This interval assigns 25% of the probability mass above and below the interval.


The tidybayes framework makes it easy to request multiple types of intervals. E.g., here we’ll request 50%, 80%, and 99% intervals.

```{r}
median_qi(samples$samples, .width = c(.5, .8, .99))
```

In contrast, the right-hand plot in Figure 3.3 displays the 50% highest posterior density interval (HPDI). The HPDI is the narrowest interval containing the specified probability mass.
Now let’s use the *mode_hdi()* function to return 50% **highest posterior density intervals (HPDIs)**.

```{r}
mode_hdi(samples$samples, .width = .5)
```

Now we have that skill, we can use it to make Figure 3.3.

```{r}
# lower left panel
d %>% 
  ggplot(aes(x = p_grid)) +
  geom_ribbon(data = d %>% filter(p_grid > median_qi(samples$samples, .width = .5)[, "ymin"] & 
                                    p_grid < median_qi(samples$samples, .width = .5)[, "ymax"]),
              aes(ymin = 0, ymax = posterior),
              fill = "grey75") +
  geom_line(aes(y = posterior)) +
  labs(subtitle = "50% Percentile Interval",
       x = "proportion of water (p)",
       y = "density")
```

```{r}
# lower right panel
d %>% 
  ggplot(aes(x = p_grid)) +
  geom_ribbon(data = d %>% filter(p_grid > median_hdi(samples$samples, .width = .5)[, "ymin"] & 
                                    p_grid < median_hdi(samples$samples, .width = .5)[, "ymax"]),
              aes(ymin = 0, ymax = posterior),
              fill = "grey75") +
  geom_line(aes(y = posterior)) +
  labs(subtitle = "50% HPDI",
       x = "proportion of water (p)",
       y = "density")
```


### 3.2.3 Point estimate
The Bayesian parameter estimate is precisely the entire posterior distribution, which is not a single number, but instead a function that maps each unique parameter value onto a plausibility value. So really the most important thing to note is that you don’t have to choose a point estimate.

But if you must produce a point estimate from the posterior, you’ll have to ask and answer more questions. Let’s consider three alternative point estimates. First, it is very common for scientists to report the parameter value with highest posterior probability, a *maximum a posteriori (MAP)* estimate.

To emphasize it, we can use slice() to select the top row.
```{r}
d %>% 
  arrange(desc(posterior)) %>% 
  slice(1)
```


And you can also do whis with *mode_hdi()* or *mode_qi()*.

```{r}
samples %>% mode_hdi(samples)
samples %>% mode_qi(samples)
```

But medians and means are typical, too.

```{r}
samples %>% 
  summarise(mean   = mean(samples),
            median = median(samples))
```



These are also point estimates, and they also summarize the posterior. But all three —the mode (MAP), mean, and median— are different in this case. How can we choose among them? We can inspect the three types of point estimate in the left panel of Figure 3.4. First we’ll bundle the three point estimates together in a tibble.

```{r}
(
  point_estimates <-
  samples %>% mean_qi(samples) %>% 
  bind_rows(
    samples %>% median_qi(samples),
    samples %>% mode_qi(samples)
  ) %>% 
  select(`samples`, `.point`) %>%  # select columns we need
  # these last two columns will help us annotate  
  mutate(x = samples + c(-.03, .03, -.03), 
         y = c(.0005, .00125, .002)) # x and y coordinate of text
)
```


One principled way to go beyond using the entire posterior as the estimate is to choose *a loss function*. A loss function is a rule that tells you the cost associated with using any particular point estimate. 

Now once you have the posterior distribution in hand, how should you use it to maximize your expected winnings? It turns out that the parameter value that maximizes expected winnings (minimizes expected loss) is the median of the posterior distribution. Please see Figure 3.4 below.

```{r}
d %>% 
  ggplot(aes(x = p_grid)) +
  geom_ribbon(aes(ymin = 0, ymax = posterior),
              fill = "grey75") +
  geom_vline(xintercept = point_estimates$samples) + # add vertivcal lines
  geom_text(data = point_estimates,
            aes(x = x, y = y, label = .point),
            angle = 90) + # turn 90 degree
  labs(x = "proportion of water (p)",
       y = "density") +  # add label
  theme(panel.grid = element_blank()) # eliminate grid line
```

Here is an example for loss function. Let `p` be the proportion of the Earth covered by water and `d` be our guess. If McElreath pays us $100 if we guess exactly right but subtracts money from the prize proportional to how far off we are, then our loss is proportional to  
`p − d`. If we decide `d = .5`, then our expected loss will be:

```{r}
d %>% 
  mutate(loss = posterior * abs(0.5 - p_grid)) %>% # our guess is .5
  summarise(`expected loss` = sum(loss)) # expected loss 

```

The symbols *posterior* and *p_grid* are the same ones we’ve been using throughout this chapter, containing the posterior probabilities and the parameter values, respectively. All the code above does is compute the weighted average loss, where each loss is weighted by its corresponding posterior probability. There’s a trick for repeating this calculation for every possible decision, using the function sapply.

```{r}
make_loss <- function(our_d){ # input: our_d
  d %>% 
    mutate(loss = posterior * abs(our_d - p_grid)) %>% 
    summarise(weighted_average_loss = sum(loss))
}

(
  l <-
  d %>% 
  select(p_grid) %>% 
  rename(decision = p_grid) %>%  # rename column
  mutate(weighted_average_loss = sapply(decision, make_loss)) %>% # calculate loss
  unnest() # change data type (list -> value)
)
```

Now we’re ready for the right panel of Figure 3.4.

```{r}
# this will help us find the x and y coordinates for the minimum value
min_loss <-
  l %>% 
  filter(weighted_average_loss == min(weighted_average_loss)) %>% 
  as.numeric()
# when our guess = median, we'll have min expected loss

# the plot
l %>%   
  ggplot(aes(x = decision)) +
  geom_ribbon(aes(ymin = 0, ymax = weighted_average_loss),
              fill = "grey75") + # fill plot
  geom_vline(xintercept = min_loss[1], color = "white", linetype = 3) + # add vertical line
  geom_hline(yintercept = min_loss[2], color = "white", linetype = 3) + # add horizontal line
  ylab("expected proportional loss") + # add ylad name
  theme(panel.grid = element_blank()) # eliminate grid line
```


## 3.3 Sampling to simulate prediction

McElreath’s four good reasons for posterior simulation were:

1. Model checking
2. Software validation
3. Research design
4. Forecasting

### 3.3.1 Dummy data
Dummy data for the globe tossing model arise from the binomial likelihood. If you let `w` be a count of water and `n` be the number of tosses, the binomial likelihood is

<size=10><center>$Pr(w|n,p)= \frac{n!}{w!(n-w)!}p^w(1-p)^{n-w}$</center>

Letting `n = 2`, `p(w) = .7`, and `w = 0 through 2`, the densities are:

```{r}
tibble(n           = 2,
       probability = .7,
       w           = 0:2) %>% 
  mutate(density   = dbinom(w, size = n, prob = probability))
```

Now we’re going to simulate observations, using these likelihoods.

```{r}
n_draws <- 1e5 # 100,000 observations

set.seed(331)
d <- tibble(draws = rbinom(n_draws, size = 9, prob = .7))

d %>% 
  group_by(draws) %>% # group observations according to their draws
  count() %>% # count the number of occurences in each group
  mutate(proportion = n / nrow(d)) # calculate the proportion of draws
```

Here’s the simulation updated so `n = 9`, which we plot in our version of Figure 3.5.

```{r}
# the histogram
d %>% 
  ggplot(aes(x = draws)) + # set x-coordinate
  geom_histogram(binwidth = 1, # width of the bins
                 center = 0, # center of the bins
                 color = "grey92", # color of the bins
                 size = 1/10) + # size of the interval
  scale_x_continuous("dummy water count", # x-coordinate name
                     breaks = seq(from = 0, to = 9, by = 2)) + # mark
  ylab("frequency") + # y-coordinate name
  coord_cartesian(xlim = 0:9) + # set xlim form 0 to 9
  theme(panel.grid = element_blank()) # eliminate grid line
```


### 3.3.2 Model checking
Model checking means **(1) ensuring the model fitting worked correctly** and **(2) evaluating the adequacy of a model for some purpose**.

### 3.3.2.1 Is the model adequate?
Let’s do some basic model checks, using simulated observations for the globe tossing model. The observations in our example case are counts of water, over tosses of the globe.

First, there is observation uncertainty. For any unique value of the parameter p, there is a unique implied pattern of observations that the model expects.

Second, there is uncertainty about p. The posterior distribution over p embodies this uncertainty. And since there is uncertainty about p, there is uncertainty about everything that depends upon p. The uncertainty in p will interact with the sampling variation, when we try to assess what the model tells us about outcomes.

If you were to compute the sampling distribution of outcomes at each value of p, then you could average all of these prediction distributions together, using the posterior probabilities of each value of p, to get a posterior predictive distribution

So how do you actually do the calculations? To simulate predicted observations for a single value of p, say p = 0.6, you can use rbinom to generate random binomial samples:

```{r}
# how many grid points would you like?
n <- 1000
n_success <- 6
n_tirals  <- 9

d <-
  tibble(p_grid     = seq(from = 0, to = 1, length.out = n),
         prior      = 1) %>% 
  mutate(likelihood = dbinom(n_success, size = n_tirals, prob = p_grid)) %>% 
  mutate(posterior  = likelihood * prior) %>% 
  mutate(posterior  = posterior / sum(posterior))

# samples!
set.seed(33.22)
samples <-
  tibble(samples = sample(d$p_grid, prob = d$posterior, size = 1e4, replace = T)) 

head(samples)
```

Let’s use it to simulate and make the middle panels of Figure 3.6.

```{r}
# the simulation
set.seed(3322)
samples <-
  samples %>% 
  mutate(w   = rbinom(1e4, size = n_tirals,  prob = samples),
         key = str_c("p = ", round(samples, digits = 1)))

# the plot
samples  %>% 
  filter(key != "p = 1") %>%
  ggplot(aes(x = w)) +
  geom_histogram(binwidth = 1, center = 0,
                 color = "grey92", size = 1/10) +
  scale_x_continuous("dummy water count",
                     breaks = seq(from = 0, to = 9, by = 3)) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = 0:9) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ key, ncol = 9, scales = "free_y") 
```

The top panel of Figure 3.6 is just the density of samples.

```{r}
samples %>% 

  ggplot(aes(x = samples)) +
  geom_density(fill = "grey50", color = "transparent") +
  scale_x_continuous("probability of water",
                     breaks = c(0, .5, 1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "Posterior probability") +
  theme(panel.grid = element_blank())
```

And the bottom panel is the histogram of w without faceting by levels of samples, combining simulated observation distributions for all parameter values (not just the ten shown), each weighted by its posterior probability, produces the posterior predictive distribution. 

```{r}
samples %>% 
  ggplot(aes(x = w)) +
  geom_histogram(binwidth = 1, center = 0,
                 color = "grey92", size = 1/10) +
  scale_x_continuous("number of water samples",
                     breaks = seq(from = 0, to = 9, by = 3)) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("Posterior predictive distribution") +
  coord_cartesian(xlim = 0:9) +
  theme(panel.grid = element_blank())
```


## 3.4 Let’s practice with stan

we’ll fit the primary model of `w = 6` and `n = 9` much like we did at the end of the project for Chapter 2.
```{r}
library(rstan)

# set up simple stan model
stanmodel =  "
data { // declares the data to be used for the model
int n; // observations
int w; // success
}
parameters { // defines the unknowns to be estimate
real p;   
}
model {
// Define the priors
p ~ uniform(0, 1); 

// The likelihood
w ~ binomial(n, p);
}
"

# run our model
data_3.1 = list(w = 6, n = 9)  # declare data
model_3.1 = stan(model_code = stanmodel, # stan program
                  data = data_3.1, # named list of data
                  cores = 1, # number of cores (could use one per chain)
                  chains = 4, # number of Markov chains
                  warmup = 1000, # number of warmup iterations per chain
                  iter = 2000) # total number of iterations per chain
```


Check our result with *extract()*.

```{r}
print(model_3.1)
fit_3.1 <- extract(model_3.1, pars="p", permuted = TRUE) %>% # extract the estimated probability of "w"
  as.tibble() %>% # make the data a tibble
  glimpse() # take a glimpse of the tibble
```

For now, notice we can view these in a density.

```{r}
fit_3.1 %>% 
  ggplot(aes(x = p)) + # set the canvas 
  geom_density(fill = "grey50", color = "transparent") +
  scale_x_continuous("probability of water",
                     breaks = c(0, .5, 1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "Posterior probability") +
  theme(panel.grid = element_blank())
```

Looks a lot like the posterior probability density at the top of Figure 3.6, doesn’t it? Much like we did with samples, we can use this distribution of probabilities to predict histograms of “w” counts.

```{r}
# the simulation
set.seed(33.22)

fit_3.1 <-
  fit_3.1 %>% 
  mutate(w   = rbinom(n(), size =  9,  prob = p)) %>% # compute "w"
  mutate(key = str_c("p = ", round(p, digits = 1))) # plot name

# the plot
fit_3.1  %>% 
  filter(key != "p = 1") %>% # filter out p = 1 
  ggplot(aes(x = w)) + # set canvas
  geom_histogram(binwidth = 1, center = 0, # plot hist.
                 color = "grey92", size = 1/10) +
  scale_x_continuous("dummy water count", # set x-coordinate
                     breaks = seq(from = 0, to = 9, by = 3)) +
  scale_y_continuous(NULL, breaks = NULL) + # set y-coordinate
  coord_cartesian(xlim = 0:9) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ key, ncol = 9, scales = "free_y") # wrap plots in one plot


# Plot posterior predictive distribution
fit_3.1 %>% 
  ggplot(aes(x = w)) +
  geom_histogram(binwidth = 1, center = 0,
                 color = "grey92", size = 1/10) +
  scale_x_continuous("number of water samples", # set x-coordinate
                     breaks = seq(from = 0, to = 9, by = 3)) +
  scale_y_continuous(NULL, breaks = NULL) + # set y-coordinate
  ggtitle("Posterior predictive distribution") +
  coord_cartesian(xlim = 0:9) +
  theme(panel.grid = element_blank())


```




